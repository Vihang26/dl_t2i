ann_root: ../examples//annotation1_blip/
batch_size_test: 32
batch_size_train: 16
image_size: 480
inference: vqa_prob
init_lr: 2e-05
max_epoch: 10
min_lr: 0
pretrained: 
  https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth
train_files: [vqa_train, vqa_val, vg_qa]
vg_root: /export/share/datasets/vision/visual-genome/
vit: base
vit_ckpt_layer: 0
vit_grad_ckpt: false
vqa_root: /export/share/datasets/vision/VQA/Images/mscoco/
weight_decay: 0.05
